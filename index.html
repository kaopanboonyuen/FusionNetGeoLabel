<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>FusionNetGeoLabel: PhD Thesis by Teerapong Panboonyuen</title>

    <!-- Favicon with globe emoji -->
    <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' width='16' height='16'><text y='14' font-size='16'>üåç</text></svg>" />

    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600&display=swap" rel="stylesheet" />
    <style>
        body {
            font-family: 'Poppins', sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f4f7fa;
            color: #333;
            animation: fadeIn 1.5s ease-in-out;
            line-height: 1.6;
        }

        @keyframes fadeIn {
            from {
                opacity: 0;
            }

            to {
                opacity: 1;
            }
        }

        header {
            background-color: #1d3557;
            color: white;
            padding: 40px 20px 30px;
            text-align: center;
            animation: headerFadeIn 2s ease-in-out;
            box-shadow: 0 3px 10px rgba(29, 53, 87, 0.4);
        }

        @keyframes headerFadeIn {
            from {
                opacity: 0;
                transform: translateY(-20px);
            }

            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        header h1 {
            font-size: 2.4rem;
            margin: 0 0 10px;
            font-weight: 600;
        }

        header .authors {
            font-size: 1rem;
            color: #a8bed6;
            margin-top: 10px;
        }

        header .authors a {
            color: #f1faee;
            text-decoration: none;
            font-weight: 600;
            margin: 0 8px;
            transition: color 0.3s;
        }

        header .authors a:hover {
            color: #e63946;
        }

        .container {
            max-width: 900px;
            margin: 30px auto 60px;
            padding: 0 20px;
        }

        section {
            margin-bottom: 40px;
            animation: sectionFadeIn 1.8s ease-in-out;
        }

        @keyframes sectionFadeIn {
            from {
                opacity: 0;
                transform: translateY(15px);
            }

            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        h2 {
            color: #1d3557;
            font-weight: 600;
            margin-bottom: 15px;
            border-bottom: 3px solid #e63946;
            padding-bottom: 6px;
        }

        h3 {
            color: #457b9d;
            margin-top: 25px;
            margin-bottom: 12px;
            font-weight: 600;
        }

        a.button {
            display: inline-block;
            padding: 10px 22px;
            background-color: #e63946;
            color: white;
            border-radius: 6px;
            text-decoration: none;
            font-weight: 600;
            transition: background-color 0.3s ease;
            margin-top: 10px;
        }

        a.button:hover {
            background-color: #f1faee;
            color: #1d3557;
        }

        p {
            margin-bottom: 15px;
            font-size: 1rem;
        }

        ul {
            margin-left: 20px;
            margin-bottom: 15px;
        }

        pre {
            background: #f1f1f1;
            padding: 12px 18px;
            border-radius: 6px;
            overflow-x: auto;
            font-family: 'Courier New', Courier, monospace;
            font-size: 0.9rem;
        }

        /* Images */
        img {
            max-width: 100%;
            border-radius: 8px;
            margin: 15px 0;
            box-shadow: 0 4px 15px rgba(29, 53, 87, 0.2);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        img:hover {
            transform: scale(1.05);
            box-shadow: 0 8px 25px rgba(29, 53, 87, 0.35);
            cursor: pointer;
        }

        footer {
            background-color: #1d3557;
            color: white;
            text-align: center;
            padding: 20px 15px;
            font-size: 0.9rem;
        }

        footer a {
            color: #e63946;
            text-decoration: none;
            font-weight: 600;
        }

        footer a:hover {
            text-decoration: underline;
        }

        hr {
            border: none;
            border-top: 1.5px solid #e63946;
            margin: 40px 0;
            width: 50px;
        }
    </style>
</head>

<body>

    <header>
        <h1>üåç FusionNetGeoLabel</h1>
        <div class="authors">
            <p>
                <strong>Ph.D. Thesis Project by</strong><br />
                <a href="https://kaopanboonyuen.github.io/" target="_blank" rel="noopener">Teerapong Panboonyuen</a> &mdash; Computer Engineering, Chulalongkorn University
            </p>
            <p>
                <a class="button" href="https://digital.car.chula.ac.th/chulaetd/8534/" target="_blank" rel="noopener">Read Full Thesis PDF</a>
                <a class="button" href="https://kaopanboonyuen.github.io/talk/ph.d.-thesis-defense/" target="_blank" rel="noopener">PhD Blog & Defense</a>
                <a class="button" href="https://github.com/kaopanboonyuen/FusionNetGeoLabel" target="_blank" rel="noopener">Source Code on GitHub</a>
            </p>
        </div>
    </header>

    <div class="container">

        <section>
            <h2>üìö Overview</h2>
            <p><strong>FusionNetGeoLabel</strong> is a cutting-edge deep learning framework tailored for semantic segmentation in remotely sensed imagery. This project is the result of my Ph.D. research focused on improving accuracy and efficiency in labeling satellite and aerial images.</p>
        </section>


       <section>
    <h2>üéì Academic Excellence & Scholarships</h2>
    <p>
        I have completed my <strong>Ph.D. in Computer Engineering</strong> at Chulalongkorn University (2018‚Äì2020), proudly supported by these prestigious scholarships:
    </p>
    <ul>
        <li><strong>The 100th Anniversary Chulalongkorn University Fund for Doctoral Scholarship</strong></li>
        <li><strong>The 90th Anniversary of Chulalongkorn University Scholarship</strong></li>
    </ul>
    <p>
        I also completed my <strong>Master of Engineering in Computer Engineering</strong> degree (2016‚Äì2017), supported by 
    </p>
    <ul>
        <li><strong>The H.M. the King Bhumibhol Adulyadej‚Äôs 72nd Birthday Anniversary Scholarship</strong></li>
    </ul>

    <h3>üìö Thesis Works</h3>
    <p>
        <a class="button" href="https://digital.car.chula.ac.th/chulaetd/8534/" target="_blank" rel="noopener" style="margin-right:10px;">
            View Ph.D. Thesis
        </a>
        <a class="button" href="https://www.car.chula.ac.th/display7.php?bib=2156287" target="_blank" rel="noopener">
            View M.Eng. Thesis
        </a>
    </p>

    <h3>üìù Selected Publications</h3>
    <ul>
        <li>
            Panboonyuen, T., et al., <em>Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images</em>, Remote Sensing, 2021.  
            <a class="button" href="https://www.mdpi.com/2072-4292/13/24/5100" target="_blank" rel="noopener" style="font-size:0.9rem; padding:5px 12px; margin-left:10px;">Read Paper</a>
        </li>
        <li>
            Panboonyuen, T., et al., <em>Feature Fusion-Based Enhanced Global Convolutional Network with Channel Attention for Remote Sensing</em>, Remote Sensing, 2020.  
            <a class="button" href="https://www.mdpi.com/2072-4292/12/8/1233" target="_blank" rel="noopener" style="font-size:0.9rem; padding:5px 12px; margin-left:10px;">Read Paper</a>
        </li>
        <li>
            Panboonyuen, T., et al., <em>Road Segmentation on Aerial Imagery Using Deep CNNs and Conditional Random Fields</em>, Remote Sensing, 2017.  
            <a class="button" href="https://www.mdpi.com/2072-4292/9/7/680" target="_blank" rel="noopener" style="font-size:0.9rem; padding:5px 12px; margin-left:10px;">Read Paper</a>
        </li>
    </ul>

    <p>
        My research advances state-of-the-art semantic segmentation for remote sensing imagery, combining novel neural architectures, attention mechanisms, and domain-specific transfer learning ‚Äî with applications in smart cities, environmental monitoring, and geospatial intelligence.
    </p>
</section>



        <section>
            <h2>üìÑ Abstract</h2>
            <p>Semantic segmentation plays a crucial role in remote sensing, impacting fields such as agriculture, map updating, and navigation.</p>
            <p>While Deep Convolutional Encoder-Decoder networks are widely used, they often struggle to accurately identify fine low-level features such as rivers and vegetation due to architectural limits and scarcity of domain-specific training data.</p>
            <p>This dissertation proposes an advanced semantic segmentation framework designed specifically for remote sensing imagery, featuring five key innovations:</p>
            <ul>
                <li><strong>Global Convolutional Network (GCN):</strong> Enhances segmentation accuracy for remote sensing images.</li>
                <li><strong>Channel Attention Mechanism:</strong> Focuses on the most critical features for better performance.</li>
                <li><strong>Domain-Specific Transfer Learning:</strong> Addresses limited training data challenges.</li>
                <li><strong>Feature Fusion (FF):</strong> Integrates low-level features effectively.</li>
                <li><strong>Depthwise Atrous Convolution (DA):</strong> Refines feature extraction for improved segmentation.</li>
            </ul>
            <p>Experiments on Landsat-8 datasets and the ISPRS Vaihingen benchmark demonstrate significant performance improvements over baseline models.</p>
        </section>

      <section>
          <h2>üìÅ Key Resources & Publications</h2>
          <p>Explore the core assets underpinning my research and contributions to the field of semantic segmentation on remote sensing imagery:</p>
          <div style="display: flex; flex-wrap: wrap; gap: 12px; margin-top: 15px;">
              <a class="button" href="https://digital.car.chula.ac.th/chulaetd/8534/" target="_blank" rel="noopener" style="flex: 1 1 250px; text-align: center;">
                  üìÑ Ph.D. Thesis PDF
              </a>
              <a class="button" href="https://kaopanboonyuen.github.io/talk/ph.d.-thesis-defense/" target="_blank" rel="noopener" style="flex: 1 1 250px; text-align: center;">
                  üìù PhD Blog & Defense
              </a>
              <a class="button" href="https://github.com/kaopanboonyuen/FusionNetGeoLabel" target="_blank" rel="noopener" style="flex: 1 1 250px; text-align: center;">
                  üíª GitHub Code Repository
              </a>
              <a class="button" href="https://www.isprs.org/resources/datasets/benchmarks/UrbanSemLab/2d-sem-label-vaihingen.aspx" target="_blank" rel="noopener" style="flex: 1 1 250px; text-align: center;">
                  üìä ISPRS Vaihingen Dataset
              </a>
              <a class="button" href="https://www.isprs.org/resources/datasets/benchmarks/UrbanSemLab/2d-sem-label-vaihingen.aspx" target="_blank" rel="noopener" style="flex: 1 1 250px; text-align: center;">
                  üèÜ ISPRS Vaihingen Leaderboard
              </a>
          </div>
          <p style="margin-top: 20px; font-style: italic; color: #555;">
              These resources highlight the rigor, reproducibility, and impact of my work within the computer vision and remote sensing communities.
          </p>
      </section>


        <section>
            <h2>üîß How to Use</h2>

            <h3>Training</h3>
            <p>Clone the repository and install dependencies:</p>
            <pre><code>git clone https://github.com/kaopanboonyuen/FusionNetGeoLabel.git
cd FusionNetGeoLabel
pip install -r requirements.txt
</code></pre>
            <p>Prepare your dataset and modify <code>config.json</code> as needed, then start training:</p>
            <pre><code>python train.py --config config.json</code></pre>

            <h3>Inference</h3>
            <p>Download pretrained models from the repository and run inference:</p>
            <pre><code>python inference.py --model path_to_pretrained_model --image path_to_image</code></pre>
        </section>

        <section>
            <h2>üìù Citation</h2>
            <p>If you use this work in your research, please cite:</p>
            <pre><code>@phdthesis{panboonyuen2019semantic,
  title     = {Semantic segmentation on remotely sensed images using deep convolutional encoder-decoder neural network},
  author    = {Teerapong Panboonyuen},
  year      = {2019},
  school    = {Chulalongkorn University},
  type      = {Ph.D. thesis},
  doi       = {10.58837/CHULA.THE.2019.158},
  address   = {Faculty of Engineering},
  note      = {Doctor of Philosophy}
}</code></pre>
        </section>

        <section>
            <h2>üì∏ Visual Results</h2>
            <p>Some highlights of our model's performance:</p>
            <img src="img/GraphicalAbstract.png" alt="Graphical Abstract" />
            <img src="img/p2_method_2.png" alt="Method Illustration 2" />
            <img src="img/p3_method_3.png" alt="Method Illustration 3" />
            <p>
              <img src="img/out3.png" alt="Sample Output 1"  />
              <img src="img/out1.png" alt="Sample Output 2" />
              <img src="img/out5.png" alt="Sample Output 3" />
            </p>
        </section>

        <section>
  <h2>üöÄ What I Do & My Impact</h2>
  <p>
    I build cutting-edge deep learning models for semantic segmentation of aerial and satellite images ‚Äî helping computers understand complex scenes like roads, vegetation, and buildings with high precision.
  </p>
  <p>
    My latest work improves on state-of-the-art by:
  </p>
  <ul>
    <li><strong>High-Resolution Backbone:</strong> Keeps detailed image features at multiple scales.</li>
    <li><strong>Feature Fusion:</strong> Combines local & global info for better accuracy.</li>
    <li><strong>Depthwise Atrous Convolution:</strong> Smart multi-scale filtering to capture fine details.</li>
  </ul>
  <p>
    Tested on top benchmarks (ISPRS Vaihingen, Landsat-8), my model scores 90%+ F1 ‚Äî outperforming previous bests and powering smarter remote sensing applications.
  </p>

</section>


        <section>
            <h2>‚öñÔ∏è License</h2>
            <p>This project is licensed under the <a href="https://opensource.org/licenses/MIT" target="_blank" rel="noopener">MIT License</a>.</p>
        </section>

    </div>

    <footer>
        <p>
            &copy; 2019 Teerapong Panboonyuen &mdash; 
            <a href="https://digital.car.chula.ac.th/chulaetd/8534/" target="_blank" rel="noopener">PhD Thesis</a>
        </p>
    </footer>

</body>

</html>
