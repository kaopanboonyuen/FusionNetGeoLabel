<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>FusionNetGeoLabel: PhD Thesis by Teerapong Panboonyuen</title>

    <!-- Favicon with globe emoji -->
    <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' width='16' height='16'><text y='14' font-size='16'>üåç</text></svg>" />

    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600&display=swap" rel="stylesheet" />
    <style>
        body {
            font-family: 'Poppins', sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f4f7fa;
            color: #333;
            animation: fadeIn 1.5s ease-in-out;
            line-height: 1.6;
        }

        @keyframes fadeIn {
            from {
                opacity: 0;
            }

            to {
                opacity: 1;
            }
        }

        header {
            background-color: #1d3557;
            color: white;
            padding: 40px 20px 30px;
            text-align: center;
            animation: headerFadeIn 2s ease-in-out;
            box-shadow: 0 3px 10px rgba(29, 53, 87, 0.4);
        }

        @keyframes headerFadeIn {
            from {
                opacity: 0;
                transform: translateY(-20px);
            }

            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

header h1 {
    font-size: 2.4rem;
    margin: 0 0 10px;
    font-weight: 600;
    word-break: break-word;     /* Allows long words to wrap */
    overflow-wrap: break-word;  /* Also supports text wrapping */
    max-width: 100%;            /* Prevents overflowing the container */
    text-align: center;
    padding: 0 10px;            /* Adds side breathing room on mobile */
}

/* üì± Responsive fix for small screens */
@media (max-width: 600px) {
    header h1 {
        font-size: 1.6rem;       /* Reduce font size on mobile */
    }
}


        header .authors {
            font-size: 1rem;
            color: #a8bed6;
            margin-top: 10px;
        }

        header .authors a {
            color: #f1faee;
            text-decoration: none;
            font-weight: 600;
            margin: 0 8px;
            transition: color 0.3s;
        }

        header .authors a:hover {
            color: #e63946;
        }

        .container {
            max-width: 900px;
            margin: 30px auto 60px;
            padding: 0 20px;
        }

        section {
            margin-bottom: 40px;
            animation: sectionFadeIn 1.8s ease-in-out;
        }

        @keyframes sectionFadeIn {
            from {
                opacity: 0;
                transform: translateY(15px);
            }

            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        h2 {
            color: #1d3557;
            font-weight: 600;
            margin-bottom: 15px;
            border-bottom: 3px solid #e63946;
            padding-bottom: 6px;
        }

        h3 {
            color: #457b9d;
            margin-top: 25px;
            margin-bottom: 12px;
            font-weight: 600;
        }

        a.button {
            display: inline-block;
            padding: 10px 22px;
            background-color: #e63946;
            color: white;
            border-radius: 6px;
            text-decoration: none;
            font-weight: 600;
            transition: background-color 0.3s ease;
            margin-top: 10px;
        }

        a.button:hover {
            background-color: #f1faee;
            color: #1d3557;
        }

        p {
            margin-bottom: 15px;
            font-size: 1rem;
        }

        ul {
            margin-left: 20px;
            margin-bottom: 15px;
        }

        pre {
            background: #f1f1f1;
            padding: 12px 18px;
            border-radius: 6px;
            overflow-x: auto;
            font-family: 'Courier New', Courier, monospace;
            font-size: 0.9rem;
        }

        /* Images */
        img {
            max-width: 100%;
            border-radius: 8px;
            margin: 15px 0;
            box-shadow: 0 4px 15px rgba(29, 53, 87, 0.2);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        img:hover {
            transform: scale(1.05);
            box-shadow: 0 8px 25px rgba(29, 53, 87, 0.35);
            cursor: pointer;
        }

        footer {
            background-color: #1d3557;
            color: white;
            text-align: center;
            padding: 20px 15px;
            font-size: 0.9rem;
        }

        footer a {
            color: #e63946;
            text-decoration: none;
            font-weight: 600;
        }

        footer a:hover {
            text-decoration: underline;
        }

        hr {
            border: none;
            border-top: 1.5px solid #e63946;
            margin: 40px 0;
            width: 50px;
        }



      
        .thesis-list {
    display: flex;
    flex-wrap: wrap;
    gap: 20px;
    margin-top: 15px;
    padding: 0 10px; /* adds spacing from screen edges on mobile */
    box-sizing: border-box;
    width: 100%;
}

.thesis-card {
    background: #ffffff;
    border-left: 5px solid #457b9d;
    padding: 20px;
    border-radius: 8px;
    box-shadow: 0 2px 10px rgba(0, 0, 0, 0.05);
    display: flex;
    flex-direction: column;
    gap: 10px;
    flex: 1 1 100%;
    box-sizing: border-box;
    width: 100%;
    max-width: 100%;
}

.thesis-card h4 {
    margin: 0;
    font-size: 1.2rem;
    color: #1d3557;
    font-weight: 600;
}

.thesis-card p {
    font-size: 0.95rem;
    color: #333;
    margin: 0;
    line-height: 1.5;
}

.thesis-card .button {
    align-self: flex-start;
    font-size: 0.9rem;
    padding: 8px 16px;
    box-sizing: border-box;
    width: fit-content;
    max-width: 100%;
    word-break: break-word;
}

/* üì± Mobile fix */
@media (max-width: 600px) {
    .thesis-list {
        flex-direction: column;
        padding: 0 16px;
    }

    .thesis-card {
        width: 100%;
    }

    .thesis-card .button {
        width: 100%;
        text-align: center;
    }
}



        .publication-list {
    display: flex;
    flex-direction: column;
    gap: 20px;
    margin: 20px 0;
    padding: 0 10px; /* prevent edge overflow on mobile */
    box-sizing: border-box;
}

.publication {
    background: #ffffff;
    border-left: 5px solid #e63946;
    padding: 15px 20px;
    border-radius: 8px;
    box-shadow: 0 2px 10px rgba(0, 0, 0, 0.05);
    display: flex;
    flex-direction: column;
    gap: 10px;
    box-sizing: border-box;
    width: 100%;
    max-width: 100%;
}

.pub-title {
    font-size: 1rem;
    margin: 0;
    color: #1d3557;
    line-height: 1.5;
}

.pub-journal {
    font-weight: 500;
    color: #457b9d;
    font-size: 0.95rem;
}

.publication .button {
    align-self: flex-start;
    font-size: 0.9rem;
    padding: 8px 16px;
    box-sizing: border-box;
    white-space: nowrap;
}

@media (max-width: 600px) {
    .publication {
        padding: 15px;
    }

    .publication .button {
        width: 100%;
        text-align: center;
        white-space: normal;
    }
}


        @media (max-width: 600px) {
    .authors p {
        display: flex;
        flex-direction: column;
        gap: 12px;
        padding: 0 10px; /* optional: adds breathing space from screen edges */
    }

    .authors a.button {
        width: 100%;
        max-width: 100%;
        box-sizing: border-box;
        text-align: center;
    }

    .publication-list {
    display: flex;
    flex-direction: column;
    gap: 20px;
    margin: 20px 0;
}


.button-row {
    display: flex;
    flex-wrap: wrap;
    gap: 15px;
}

.button-row .button {
    flex: 1 1 45%; /* Ensures buttons are responsive */
    text-align: center;
}



}


    </style>
</head>

<body>

    <header>
        <h1>üåç FusionNetGeoLabel</h1>
        <div class="authors">
            <p>
                <strong>Ph.D. Thesis Project by</strong><br />
                <a href="https://kaopanboonyuen.github.io/" target="_blank" rel="noopener">Teerapong Panboonyuen</a> &mdash; Chulalongkorn University
            </p>
            <p>
                <a class="button" href="https://digital.car.chula.ac.th/chulaetd/8534/" target="_blank" rel="noopener">Read Full Thesis PDF</a>
                <a class="button" href="https://kaopanboonyuen.github.io/talk/ph.d.-thesis-defense/" target="_blank" rel="noopener">PhD Blog & Defense</a>
                <a class="button" href="https://github.com/kaopanboonyuen/FusionNetGeoLabel" target="_blank" rel="noopener">Source Code on GitHub</a>
            </p>
        </div>
    </header>

    <div class="container">

       <section>
    <h2>üìö Project Overview</h2>
    <p>
        <strong>FusionNetGeoLabel</strong> is an advanced deep learning framework designed for semantic segmentation in satellite and aerial imagery. Developed as part of my Ph.D. research, it aims to significantly enhance the accuracy and efficiency of geospatial labeling using cutting-edge neural architectures, attention mechanisms, and domain-specific transfer learning techniques.
    </p>
</section>


<section>
    <h2>üéì Academic Journey & Scholarships</h2>
    <p>
        I received my <strong>Ph.D. in Computer Engineering</strong> from Chulalongkorn University (2018‚Äì2020), supported by two prestigious scholarships:
    </p>
    <ul>
        <li><strong>The 100th Anniversary Chulalongkorn University Fund for Doctoral Scholarship</strong></li>
        <li><strong>The 90th Anniversary of Chulalongkorn University Scholarship</strong></li>
    </ul>

    <p>
        Prior to this, I received my <strong>Master of Engineering in Computer Engineering</strong> from Chulalongkorn University (2016‚Äì2017), supported by:
    </p>
    <ul>
        <li><strong>H.M. the King Bhumibol Adulyadej‚Äôs 72nd Birthday Anniversary Scholarship</strong></li>
    </ul>

    <h3>üìñ Thesis Works</h3>
    <div class="thesis-list">

        <div class="thesis-card">
            <h4>Ph.D. Thesis</h4>
            <p>
                <strong>FusionNetGeoLabel:</strong> A Deep Learning Framework for Semantic Segmentation in Remote Sensing.<br>
                <em>Teerapong Panboonyuen</em><br>
                <em>Chulalongkorn University, 2020</em>
            </p>
            <!-- <a class="button" href="https://digital.car.chula.ac.th/chulaetd/8534/" target="_blank" rel="noopener">üìÑ Read Ph.D. Thesis</a> -->
            
            <div class="button-row">
                <a class="button" href="https://digital.car.chula.ac.th/chulaetd/8534/" target="_blank" rel="noopener">üé§ Ph.D. Thesis</a>
                <a class="button" href="https://kaopanboonyuen.github.io/files/panboonyuen_phd_defense_2020.pdf" target="_blank" rel="noopener">üîç Ph.D. Slides</a>
                <a class="button" href="https://kaopanboonyuen.github.io/talk/ph.d.-thesis-defense/" target="_blank" rel="noopener">üé§ Thesis Blog</a>
                <a class="button" href="https://kaopanboonyuen.github.io/FusionNetGeoLabel/" target="_blank" rel="noopener">üîç Showcase</a>
            </div>
<!-- 
            <div class="button-row">
                <a class="button" href="https://kaopanboonyuen.github.io/talk/ph.d.-thesis-defense/" target="_blank" rel="noopener">üé§ Thesis Blog</a>
                <a class="button" href="https://kaopanboonyuen.github.io/FusionNetGeoLabel/" target="_blank" rel="noopener">üîç Thesis Showcase</a>
            </div> -->



            <!-- <div class="button-row">
                <a class="button" href="https://kaopanboonyuen.github.io/files/panboonyuen_phd_defense_2020.pdf" target="_blank" rel="noopener">üìë Thesis Slides</a>
            </div> -->
        </div>

        <div class="thesis-card">
            <h4>M.Eng. Thesis</h4>
            <p>
                <strong>High-Resolution Road Extraction:</strong> Using Deep Convolutional Neural Networks and CRFs.<br>
               <em>Teerapong Panboonyuen</em><br>
                <em>Chulalongkorn University, 2017</em>
            </p>
            <a class="button" href="https://www.car.chula.ac.th/display7.php?bib=2156287" target="_blank" rel="noopener">üìÑ Read M.Eng. Thesis</a>
        </div>

    </div>

    <h3>üìù Selected Publications</h3>
    <div class="publication-list">

        <div class="publication">
            <p class="pub-title">
                <strong>Panboonyuen, T.</strong>, et al.<br />
                <em>Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images</em><br />
                <span class="pub-journal">Remote Sensing, 2021</span>
            </p>
            <a class="button" href="https://www.mdpi.com/2072-4292/13/24/5100" target="_blank" rel="noopener">üìÑ Read Paper</a>
        </div>

        <div class="publication">
            <p class="pub-title">
                <strong>Panboonyuen, T.</strong>, et al.<br />
                <em>Feature Fusion-Based Enhanced Global Convolutional Network with Channel Attention for Remote Sensing</em><br />
                <span class="pub-journal">Remote Sensing, 2020</span>
            </p>
            <a class="button" href="https://www.mdpi.com/2072-4292/12/8/1233" target="_blank" rel="noopener">üìÑ Read Paper</a>
        </div>

        <div class="publication">
            <p class="pub-title">
                <strong>Panboonyuen, T.</strong>, et al.<br />
                <em>Semantic Segmentation on Remotely Sensed Images Using an Enhanced Global Convolutional Network with Channel Attention and Domain Specific Transfer Learning</em><br />
                <span class="pub-journal">Remote Sensing, 2019</span>
            </p>
            <a class="button" href="https://www.mdpi.com/2072-4292/11/1/83" target="_blank" rel="noopener">üìÑ Read Paper</a>
        </div>

        <div class="publication">
            <p class="pub-title">
                <strong>Panboonyuen, T.</strong>, et al.<br />
                <em>Road Segmentation on Aerial Imagery Using Deep CNNs and Conditional Random Fields</em><br />
                <span class="pub-journal">Remote Sensing, 2017</span>
            </p>
            <a class="button" href="https://www.mdpi.com/2072-4292/9/7/680" target="_blank" rel="noopener">üìÑ Read Paper</a>
        </div>

    </div>

    <p>
        My research contributes to the advancement of intelligent systems in geospatial analysis ‚Äî supporting smart cities, environmental monitoring, disaster response, and geospatial intelligence with more robust and accurate semantic segmentation models.
    </p>
</section>




        <section>
            <h2>üìÑ Abstract</h2>
            <p>Semantic segmentation plays a crucial role in remote sensing, impacting fields such as agriculture, map updating, and navigation.</p>
            <p>While Deep Convolutional Encoder-Decoder networks are widely used, they often struggle to accurately identify fine low-level features such as rivers and vegetation due to architectural limits and scarcity of domain-specific training data.</p>
            <p>This dissertation proposes an advanced semantic segmentation framework designed specifically for remote sensing imagery, featuring five key innovations:</p>
            <ul>
                <li><strong>Global Convolutional Network (GCN):</strong> Enhances segmentation accuracy for remote sensing images.</li>
                <li><strong>Channel Attention Mechanism:</strong> Focuses on the most critical features for better performance.</li>
                <li><strong>Domain-Specific Transfer Learning:</strong> Addresses limited training data challenges.</li>
                <li><strong>Feature Fusion (FF):</strong> Integrates low-level features effectively.</li>
                <li><strong>Depthwise Atrous Convolution (DA):</strong> Refines feature extraction for improved segmentation.</li>
            </ul>
            <p>Experiments on Landsat-8 datasets and the ISPRS Vaihingen benchmark demonstrate significant performance improvements over baseline models.</p>
        </section>

      <section>
          <h2>üìÅ Key Resources & Publications</h2>
          <p>Explore the core assets underpinning my research and contributions to the field of semantic segmentation on remote sensing imagery:</p>
          <div style="display: flex; flex-wrap: wrap; gap: 12px; margin-top: 15px;">
              <a class="button" href="https://digital.car.chula.ac.th/chulaetd/8534/" target="_blank" rel="noopener" style="flex: 1 1 250px; text-align: center;">
                  üìÑ Ph.D. Thesis PDF
              </a>
              <a class="button" href="https://kaopanboonyuen.github.io/talk/ph.d.-thesis-defense/" target="_blank" rel="noopener" style="flex: 1 1 250px; text-align: center;">
                  üìù PhD Blog & Defense
              </a>
              <a class="button" href="https://github.com/kaopanboonyuen/FusionNetGeoLabel" target="_blank" rel="noopener" style="flex: 1 1 250px; text-align: center;">
                  üíª GitHub Code Repository
              </a>
              <a class="button" href="https://www.isprs.org/resources/datasets/benchmarks/UrbanSemLab/2d-sem-label-vaihingen.aspx" target="_blank" rel="noopener" style="flex: 1 1 250px; text-align: center;">
                  üìä ISPRS Vaihingen Dataset
              </a>
              <a class="button" href="https://www.isprs.org/resources/datasets/benchmarks/UrbanSemLab/2d-sem-label-vaihingen.aspx" target="_blank" rel="noopener" style="flex: 1 1 250px; text-align: center;">
                  üèÜ ISPRS Vaihingen Leaderboard
              </a>
          </div>
          <p style="margin-top: 20px; font-style: italic; color: #555;">
              These resources highlight the rigor, reproducibility, and impact of my work within the computer vision and remote sensing communities.
          </p>
      </section>


        <section>
            <h2>üîß How to Use</h2>

            <h3>Training</h3>
            <p>Clone the repository and install dependencies:</p>
            <pre><code>git clone https://github.com/kaopanboonyuen/FusionNetGeoLabel.git
cd FusionNetGeoLabel
pip install -r requirements.txt
</code></pre>
            <p>Prepare your dataset and modify <code>config.json</code> as needed, then start training:</p>
            <pre><code>python train.py --config config.json</code></pre>

            <h3>Inference</h3>
            <p>Download pretrained models from the repository and run inference:</p>
            <pre><code>python inference.py --model path_to_pretrained_model --image path_to_image</code></pre>
        </section>

        <section>
            <h2>üìù Citation</h2>
            <p>If you use this work in your research, please cite:</p>
            <pre><code>@phdthesis{panboonyuen2019semantic,
  title     = {Semantic segmentation on remotely sensed images using deep convolutional encoder-decoder neural network},
  author    = {Teerapong Panboonyuen},
  year      = {2019},
  school    = {Chulalongkorn University},
  type      = {Ph.D. thesis},
  doi       = {10.58837/CHULA.THE.2019.158},
  address   = {Faculty of Engineering},
  note      = {Doctor of Philosophy}
}</code></pre>
        </section>

        <section>
            <h2>üì∏ Visual Results</h2>
            <p>Some highlights of our model's performance:</p>
            <img src="img/GraphicalAbstract.png" alt="Graphical Abstract" />
            <img src="img/p2_method_2.png" alt="Method Illustration 2" />
            <img src="img/p3_method_3.png" alt="Method Illustration 3" />
            <p>
              <img src="img/out3.png" alt="Sample Output 1"  />
              <img src="img/out1.png" alt="Sample Output 2" />
              <img src="img/out5.png" alt="Sample Output 3" />
            </p>
        </section>

        <section>
  <h2>üöÄ What I Do & My Impact</h2>
  <p>
    I build cutting-edge deep learning models for semantic segmentation of aerial and satellite images ‚Äî helping computers understand complex scenes like roads, vegetation, and buildings with high precision.
  </p>
  <p>
    My latest work improves on state-of-the-art by:
  </p>
  <ul>
    <li><strong>High-Resolution Backbone:</strong> Keeps detailed image features at multiple scales.</li>
    <li><strong>Feature Fusion:</strong> Combines local & global info for better accuracy.</li>
    <li><strong>Depthwise Atrous Convolution:</strong> Smart multi-scale filtering to capture fine details.</li>
  </ul>
  <p>
    Tested on top benchmarks (ISPRS Vaihingen, Landsat-8), my model scores 90%+ F1 ‚Äî outperforming previous bests and powering smarter remote sensing applications.
  </p>

</section>


        <section>
            <h2>‚öñÔ∏è License</h2>
            <p>This project is licensed under the <a href="https://opensource.org/licenses/MIT" target="_blank" rel="noopener">MIT License</a>.</p>
        </section>

    </div>

    <footer>
        <p>
            &copy; 2020 Teerapong Panboonyuen</a>
        </p>
    </footer>

</body>

</html>
